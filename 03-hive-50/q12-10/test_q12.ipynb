{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext bigdata\n",
    "%hive_start\n",
    "%timeout 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS t0;\n",
      "OK\n",
      "Time taken: 10.217 seconds\n",
      "CREATE TABLE t0 (\n",
      "    c1 STRING,\n",
      "    c2 ARRAY<CHAR(1)>, \n",
      "    c3 MAP<STRING, INT>\n",
      "    )\n",
      "    ROW FORMAT DELIMITED \n",
      "        FIELDS TERMINATED BY '\\t'\n",
      "        COLLECTION ITEMS TERMINATED BY ','\n",
      "        MAP KEYS TERMINATED BY '#'\n",
      "        LINES TERMINATED BY '\\n';\n",
      "OK\n",
      "Time taken: 0.701 seconds\n",
      "LOAD DATA LOCAL INPATH 'data.tsv' INTO TABLE t0;\n",
      "Loading data to table default.t0\n",
      "OK\n",
      "Time taken: 1.342 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DROP TABLE IF EXISTS t0;\n",
    "CREATE TABLE t0 (\n",
    "    c1 STRING,\n",
    "    c2 ARRAY<CHAR(1)>, \n",
    "    c3 MAP<STRING, INT>\n",
    "    )\n",
    "    ROW FORMAT DELIMITED \n",
    "        FIELDS TERMINATED BY '\\t'\n",
    "        COLLECTION ITEMS TERMINATED BY ','\n",
    "        MAP KEYS TERMINATED BY '#'\n",
    "        LINES TERMINATED BY '\\n';\n",
    "LOAD DATA LOCAL INPATH 'data.tsv' INTO TABLE t0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM t0 LIMIT 5;\n",
      "OK\n",
      "E\t[\"b\",\"g\",\"f\"]\t{\"jjj\":3,\"bbb\":0,\"ddd\":9,\"ggg\":8,\"hhh\":2}\n",
      "A\t[\"a\",\"f\",\"c\"]\t{\"ccc\":2,\"ddd\":0,\"aaa\":3,\"hhh\":9}\n",
      "B\t[\"f\",\"e\",\"a\",\"c\"]\t{\"ddd\":2,\"ggg\":5,\"ccc\":6,\"jjj\":1}\n",
      "A\t[\"a\",\"b\"]\t{\"hhh\":9,\"iii\":5,\"eee\":7,\"bbb\":1}\n",
      "C\t[\"f\",\"g\",\"d\",\"a\"]\t{\"iii\":6,\"ddd\":5,\"eee\":4,\"jjj\":3}\n",
      "Time taken: 0.708 seconds, Fetched: 5 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive \n",
    "SELECT * FROM t0 LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS dataT; \n",
      "OK\n",
      "Time taken: 0.1 seconds\n",
      "CREATE TABLE dataT (c2 ARRAY<CHAR(1)>, registro STRING);\n",
      "OK\n",
      "Time taken: 0.051 seconds\n",
      "INSERT OVERWRITE TABLE dataT\n",
      "SELECT\n",
      "    c2,\n",
      "    clave\n",
      "FROM\n",
      "    t0\n",
      "LATERAL VIEW\n",
      "    explode(c3) t0 AS clave,valor;\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = root_20200228163258_e5b89ba2-0c7d-4968-b7d7-224efd05baf1\n",
      "Total jobs = 3\n",
      "Launching Job 1 out of 3\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1582902829190_0022, Tracking URL = http://5c70801abc21:8088/proxy/application_1582902829190_0022/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1582902829190_0022\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2020-02-28 16:33:07,066 Stage-1 map = 0%,  reduce = 0%\n",
      "2020-02-28 16:33:12,723 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.62 sec\n",
      "MapReduce Total cumulative CPU time: 1 seconds 620 msec\n",
      "Ended Job = job_1582902829190_0022\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to directory hdfs://0.0.0.0:9000/user/hive/warehouse/datat/.hive-staging_hive_2020-02-28_16-32-58_430_413312896390467206-1/-ext-10000\n",
      "Loading data to table default.datat\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 1.62 sec   HDFS Read: 6994 HDFS Write: 1772 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 1 seconds 620 msec\n",
      "OK\n",
      "Time taken: 16.091 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive \n",
    "DROP TABLE IF EXISTS dataT; \n",
    "CREATE TABLE dataT (c2 ARRAY<CHAR(1)>, registro STRING);\n",
    "INSERT OVERWRITE TABLE dataT\n",
    "SELECT\n",
    "    c2,\n",
    "    clave\n",
    "FROM\n",
    "    t0\n",
    "LATERAL VIEW\n",
    "    explode(c3) t0 AS clave,valor;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM dataT LIMIT 5;\n",
      "OK\n",
      "[\"b\",\"g\",\"f\"]\tjjj\n",
      "[\"b\",\"g\",\"f\"]\tbbb\n",
      "[\"b\",\"g\",\"f\"]\tddd\n",
      "[\"b\",\"g\",\"f\"]\tggg\n",
      "[\"b\",\"g\",\"f\"]\thhh\n",
      "Time taken: 0.163 seconds, Fetched: 5 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive \n",
    "SELECT * FROM dataT LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS dataTable; \n",
      "OK\n",
      "Time taken: 0.081 seconds\n",
      "CREATE TABLE dataTable (letra STRING, registro STRING);\n",
      "OK\n",
      "Time taken: 0.049 seconds\n",
      "INSERT OVERWRITE TABLE dataTable\n",
      "SELECT\n",
      "    letra,\n",
      "    registro\n",
      "FROM\n",
      "    dataT\n",
      "LATERAL VIEW\n",
      "    explode(c2) dataT AS letra\n",
      "ORDER BY\n",
      "    letra,\n",
      "    registro;\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = root_20200228163805_5cfcdf39-288f-49c0-bf0d-6ce490454d90\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1582902829190_0025, Tracking URL = http://5c70801abc21:8088/proxy/application_1582902829190_0025/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1582902829190_0025\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2020-02-28 16:38:12,307 Stage-1 map = 0%,  reduce = 0%\n",
      "2020-02-28 16:38:17,799 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.82 sec\n",
      "2020-02-28 16:38:24,342 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.79 sec\n",
      "MapReduce Total cumulative CPU time: 4 seconds 790 msec\n",
      "Ended Job = job_1582902829190_0025\n",
      "Loading data to table default.datatable\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.79 sec   HDFS Read: 12202 HDFS Write: 3076 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 4 seconds 790 msec\n",
      "OK\n",
      "Time taken: 20.743 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive \n",
    "DROP TABLE IF EXISTS dataTable; \n",
    "CREATE TABLE dataTable (letra STRING, registro STRING);\n",
    "INSERT OVERWRITE TABLE dataTable\n",
    "SELECT\n",
    "    letra,\n",
    "    registro\n",
    "FROM\n",
    "    dataT\n",
    "LATERAL VIEW\n",
    "    explode(c2) dataT AS letra\n",
    "ORDER BY\n",
    "    letra,\n",
    "    registro;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM dataTable LIMIT 15;\n",
      "OK\n",
      "a\taaa\n",
      "a\taaa\n",
      "a\taaa\n",
      "a\taaa\n",
      "a\taaa\n",
      "a\tbbb\n",
      "a\tbbb\n",
      "a\tbbb\n",
      "a\tbbb\n",
      "a\tbbb\n",
      "a\tbbb\n",
      "a\tbbb\n",
      "a\tccc\n",
      "a\tccc\n",
      "a\tccc\n",
      "Time taken: 13.221 seconds, Fetched: 15 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive \n",
    "SELECT * FROM dataTable LIMIT 15;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS word_counts;\n",
      "OK\n",
      "Time taken: 0.06 seconds\n",
      "CREATE TABLE word_counts\n",
      "AS\n",
      "    SELECT word2, word1, count(1) AS count\n",
      "    FROM\n",
      "        (SELECT letra AS word1, registro AS word2 FROM dataTable) w\n",
      "GROUP BY\n",
      "    word1, word2\n",
      "ORDER BY\n",
      "    word1;\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = root_20200228165004_2cbaed11-010b-4f2c-bf22-b22baa8768ac\n",
      "Total jobs = 2\n",
      "Launching Job 1 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1582902829190_0030, Tracking URL = http://5c70801abc21:8088/proxy/application_1582902829190_0030/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1582902829190_0030\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2020-02-28 16:50:12,793 Stage-1 map = 0%,  reduce = 0%\n",
      "2020-02-28 16:50:19,334 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.1 sec\n",
      "2020-02-28 16:50:25,921 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.13 sec\n",
      "MapReduce Total cumulative CPU time: 4 seconds 130 msec\n",
      "Ended Job = job_1582902829190_0030\n",
      "Launching Job 2 out of 2\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1582902829190_0031, Tracking URL = http://5c70801abc21:8088/proxy/application_1582902829190_0031/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1582902829190_0031\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2020-02-28 16:50:39,569 Stage-2 map = 0%,  reduce = 0%\n",
      "2020-02-28 16:50:45,047 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.44 sec\n",
      "2020-02-28 16:50:51,619 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 4.13 sec\n",
      "MapReduce Total cumulative CPU time: 4 seconds 130 msec\n",
      "Ended Job = job_1582902829190_0031\n",
      "Moving data to directory hdfs://0.0.0.0:9000/user/hive/warehouse/word_counts\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.13 sec   HDFS Read: 10833 HDFS Write: 1776 SUCCESS\n",
      "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 4.13 sec   HDFS Read: 7032 HDFS Write: 649 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 8 seconds 260 msec\n",
      "OK\n",
      "Time taken: 49.422 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DROP TABLE IF EXISTS word_counts;\n",
    "CREATE TABLE word_counts\n",
    "AS\n",
    "    SELECT word2, word1, count(1) AS count\n",
    "    FROM\n",
    "        (SELECT letra AS word1, registro AS word2 FROM dataTable) w\n",
    "GROUP BY\n",
    "    word1, word2\n",
    "ORDER BY\n",
    "    word1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT word1, word2, count FROM word_counts ORDER BY word1, word2;\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = root_20200228165323_0eb79880-68b5-4a01-a07d-dc0a3184ef08\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1582902829190_0033, Tracking URL = http://5c70801abc21:8088/proxy/application_1582902829190_0033/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1582902829190_0033\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2020-02-28 16:53:31,763 Stage-1 map = 0%,  reduce = 0%\n",
      "2020-02-28 16:53:37,370 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.54 sec\n",
      "2020-02-28 16:53:42,839 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.47 sec\n",
      "MapReduce Total cumulative CPU time: 3 seconds 470 msec\n",
      "Ended Job = job_1582902829190_0033\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.47 sec   HDFS Read: 8076 HDFS Write: 1500 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 3 seconds 470 msec\n",
      "OK\n",
      "a\taaa\t5\n",
      "a\tbbb\t7\n",
      "a\tccc\t13\n",
      "a\tddd\t13\n",
      "a\teee\t7\n",
      "a\tfff\t10\n",
      "a\tggg\t8\n",
      "a\thhh\t8\n",
      "a\tiii\t7\n",
      "a\tjjj\t10\n",
      "b\taaa\t4\n",
      "b\tbbb\t7\n",
      "b\tccc\t5\n",
      "b\tddd\t7\n",
      "b\teee\t5\n",
      "b\tfff\t8\n",
      "b\tggg\t4\n",
      "b\thhh\t7\n",
      "b\tiii\t7\n",
      "b\tjjj\t5\n",
      "c\taaa\t5\n",
      "c\tbbb\t4\n",
      "c\tccc\t12\n",
      "c\tddd\t9\n",
      "c\teee\t6\n",
      "c\tfff\t8\n",
      "c\tggg\t7\n",
      "c\thhh\t7\n",
      "c\tiii\t8\n",
      "c\tjjj\t8\n",
      "d\taaa\t4\n",
      "d\tbbb\t6\n",
      "d\tccc\t7\n",
      "d\tddd\t5\n",
      "d\teee\t6\n",
      "d\tfff\t8\n",
      "d\tggg\t6\n",
      "d\thhh\t4\n",
      "d\tiii\t9\n",
      "d\tjjj\t8\n",
      "e\taaa\t3\n",
      "e\tbbb\t8\n",
      "e\tccc\t9\n",
      "e\tddd\t7\n",
      "e\teee\t7\n",
      "e\tfff\t9\n",
      "e\tggg\t4\n",
      "e\thhh\t4\n",
      "e\tiii\t8\n",
      "e\tjjj\t3\n",
      "f\taaa\t8\n",
      "f\tbbb\t10\n",
      "f\tccc\t13\n",
      "f\tddd\t17\n",
      "f\teee\t11\n",
      "f\tfff\t11\n",
      "f\tggg\t9\n",
      "f\thhh\t10\n",
      "f\tiii\t10\n",
      "f\tjjj\t12\n",
      "g\taaa\t3\n",
      "g\tbbb\t3\n",
      "g\tccc\t6\n",
      "g\tddd\t5\n",
      "g\teee\t4\n",
      "g\tfff\t5\n",
      "g\tggg\t4\n",
      "g\thhh\t3\n",
      "g\tiii\t4\n",
      "g\tjjj\t6\n",
      "Time taken: 21.538 seconds, Fetched: 70 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT word1, word2, count FROM word_counts ORDER BY word1, word2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "%hive_quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "drwxrwx---   - root supergroup          0 2020-02-28 15:14 /tmp/hadoop-yarn\n",
      "drwxrwxrwx   - root supergroup          0 2020-02-28 15:19 /tmp/hive\n",
      "drwxrwxrwx   - root supergroup          0 2020-02-28 16:20 /tmp/output\n",
      "drwxr-xr-x   - root supergroup          0 2020-02-28 15:33 /tmp/wordcount\n"
     ]
    }
   ],
   "source": [
    "## Copia Archivos a sistema HDFS\n",
    "##\n",
    "## Se usan un directorio temporal en el HDFS. La siguiente\n",
    "## instrucción muestra el contenido del dicho directorio\n",
    "##\n",
    "!hdfs dfs -ls /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/tmp/wordcount': File exists\n"
     ]
    }
   ],
   "source": [
    "## Crea la carpeta wordcount en el hdfs\n",
    "##\n",
    "!hdfs dfs -mkdir /tmp/wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "drwxrwx---   - root supergroup          0 2020-02-28 15:14 /tmp/hadoop-yarn\n",
      "drwxrwxrwx   - root supergroup          0 2020-02-28 15:19 /tmp/hive\n",
      "drwxrwxrwx   - root supergroup          0 2020-02-28 16:20 /tmp/output\n",
      "drwxr-xr-x   - root supergroup          0 2020-02-28 15:33 /tmp/wordcount\n"
     ]
    }
   ],
   "source": [
    "## Verifica la creación de la carpeta\n",
    "##\n",
    "!hdfs dfs -ls /tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `/tmp/wordcount/data.tsv': File exists\n"
     ]
    }
   ],
   "source": [
    "## Copia los archvios del directorio local wordcount/\n",
    "## al directorio /tmp/wordcount/ en el hdfs\n",
    "##\n",
    "!hdfs dfs -copyFromLocal 'data.tsv'  /tmp/wordcount/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   1 root supergroup       1394 2020-02-28 15:33 /tmp/wordcount/data.tsv\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Verifica que los archivos esten copiados\n",
    "## en el hdfs\n",
    "##\n",
    "!hdfs dfs -ls /tmp/wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "## Generación del script y ajuste del código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount.hql\n",
    "\n",
    "DROP TABLE IF EXISTS t0;\n",
    "CREATE TABLE t0 (\n",
    "    c1 STRING,\n",
    "    c2 ARRAY<CHAR(1)>, \n",
    "    c3 MAP<STRING, INT>\n",
    "    )\n",
    "    ROW FORMAT DELIMITED \n",
    "        FIELDS TERMINATED BY '\\t'\n",
    "        COLLECTION ITEMS TERMINATED BY ','\n",
    "        MAP KEYS TERMINATED BY '#'\n",
    "        LINES TERMINATED BY '\\n';\n",
    "LOAD DATA LOCAL INPATH 'data.tsv' INTO TABLE t0;\n",
    "\n",
    "\n",
    "DROP TABLE IF EXISTS dataT; \n",
    "CREATE TABLE dataT (c2 ARRAY<CHAR(1)>, registro STRING);\n",
    "INSERT OVERWRITE TABLE dataT\n",
    "SELECT\n",
    "    c2,\n",
    "    clave\n",
    "FROM\n",
    "    t0\n",
    "LATERAL VIEW\n",
    "    explode(c3) t0 AS clave,valor;\n",
    "    \n",
    "    \n",
    "DROP TABLE IF EXISTS dataTable; \n",
    "CREATE TABLE dataTable (letra STRING, registro STRING);\n",
    "INSERT OVERWRITE TABLE dataTable\n",
    "SELECT\n",
    "    letra,\n",
    "    registro\n",
    "FROM\n",
    "    dataT\n",
    "LATERAL VIEW\n",
    "    explode(c2) dataT AS letra\n",
    "ORDER BY\n",
    "    letra,\n",
    "    registro;\n",
    "    \n",
    "DROP TABLE IF EXISTS word_counts;\n",
    "CREATE TABLE word_counts\n",
    "AS\n",
    "    SELECT word2, word1, count(1) AS count\n",
    "    FROM\n",
    "        (SELECT letra AS word1, registro AS word2 FROM dataTable) w\n",
    "GROUP BY\n",
    "    word1, word2\n",
    "ORDER BY\n",
    "    word1;\n",
    "    \n",
    "INSERT OVERWRITE DIRECTORY '/tmp/output'\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "\n",
    "SELECT word1, word2, count FROM word_counts ORDER BY word1, word2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n"
     ]
    }
   ],
   "source": [
    "## Ejecucion\n",
    "!hive -S -e 'source wordcount.hql'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/000000_0\n"
     ]
    }
   ],
   "source": [
    "### Copia de los resultados a la maquina local (vagrant)\n",
    "!hadoop fs -copyToLocal /tmp/output output\n",
    "!ls output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a,aaa,5\n",
      "a,bbb,7\n",
      "a,ccc,13\n",
      "a,ddd,13\n",
      "a,eee,7\n",
      "a,fff,10\n",
      "a,ggg,8\n",
      "a,hhh,8\n",
      "a,iii,7\n",
      "a,jjj,10\n"
     ]
    }
   ],
   "source": [
    "!cat output/000000_0 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
