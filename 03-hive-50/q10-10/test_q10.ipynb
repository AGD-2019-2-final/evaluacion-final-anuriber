{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext bigdata\n",
    "%hive_start\n",
    "%timeout 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS t0;\n",
      "OK\n",
      "Time taken: 16.564 seconds\n",
      "CREATE TABLE t0 (\n",
      "    c1 STRING,\n",
      "    c2 ARRAY<CHAR(1)>, \n",
      "    c3 MAP<STRING, INT>\n",
      "    )\n",
      "    ROW FORMAT DELIMITED \n",
      "        FIELDS TERMINATED BY '\\t'\n",
      "        COLLECTION ITEMS TERMINATED BY ','\n",
      "        MAP KEYS TERMINATED BY '#'\n",
      "        LINES TERMINATED BY '\\n';\n",
      "OK\n",
      "Time taken: 1.327 seconds\n",
      "LOAD DATA LOCAL INPATH 'data.tsv' INTO TABLE t0;\n",
      "Loading data to table default.t0\n",
      "OK\n",
      "Time taken: 2.485 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DROP TABLE IF EXISTS t0;\n",
    "CREATE TABLE t0 (\n",
    "    c1 STRING,\n",
    "    c2 ARRAY<CHAR(1)>, \n",
    "    c3 MAP<STRING, INT>\n",
    "    )\n",
    "    ROW FORMAT DELIMITED \n",
    "        FIELDS TERMINATED BY '\\t'\n",
    "        COLLECTION ITEMS TERMINATED BY ','\n",
    "        MAP KEYS TERMINATED BY '#'\n",
    "        LINES TERMINATED BY '\\n';\n",
    "LOAD DATA LOCAL INPATH 'data.tsv' INTO TABLE t0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM t0;\n",
      "OK\n",
      "E\t[\"b\",\"g\",\"f\"]\t{\"jjj\":3,\"bbb\":0,\"ddd\":9,\"ggg\":8,\"hhh\":2}\n",
      "A\t[\"a\",\"f\",\"c\"]\t{\"ccc\":2,\"ddd\":0,\"aaa\":3,\"hhh\":9}\n",
      "B\t[\"f\",\"e\",\"a\",\"c\"]\t{\"ddd\":2,\"ggg\":5,\"ccc\":6,\"jjj\":1}\n",
      "A\t[\"a\",\"b\"]\t{\"hhh\":9,\"iii\":5,\"eee\":7,\"bbb\":1}\n",
      "C\t[\"f\",\"g\",\"d\",\"a\"]\t{\"iii\":6,\"ddd\":5,\"eee\":4,\"jjj\":3}\n",
      "A\t[\"c\",\"d\"]\t{\"bbb\":2,\"hhh\":0,\"ccc\":4,\"fff\":1,\"aaa\":7}\n",
      "A\t[\"g\",\"d\",\"a\"]\t{\"aaa\":5,\"fff\":8,\"ddd\":2,\"iii\":0,\"jjj\":7,\"ccc\":1}\n",
      "B\t[\"b\",\"a\"]\t{\"fff\":3,\"hhh\":1,\"ddd\":2}\n",
      "E\t[\"d\",\"e\",\"a\",\"f\"]\t{\"eee\":4,\"ccc\":5,\"iii\":9,\"fff\":7,\"ggg\":6,\"bbb\":0}\n",
      "B\t[\"d\",\"b\",\"g\",\"f\"]\t{\"bbb\":7,\"jjj\":9,\"fff\":5,\"iii\":4,\"ggg\":2,\"eee\":3}\n",
      "C\t[\"d\",\"c\",\"f\",\"b\"]\t{\"hhh\":6,\"eee\":4,\"iii\":0,\"fff\":2,\"jjj\":1}\n",
      "C\t[\"d\",\"e\",\"a\",\"c\"]\t{\"bbb\":7,\"iii\":6,\"ggg\":9}\n",
      "D\t[\"g\",\"e\",\"f\",\"b\"]\t{\"bbb\":9,\"aaa\":3,\"ccc\":6,\"fff\":4,\"eee\":2}\n",
      "E\t[\"c\",\"f\"]\t{\"aaa\":8,\"ddd\":5,\"jjj\":1}\n",
      "B\t[\"d\",\"b\"]\t{\"ccc\":0,\"jjj\":6,\"fff\":7,\"ddd\":3,\"aaa\":2}\n",
      "D\t[\"f\",\"e\"]\t{\"ccc\":0,\"eee\":6,\"bbb\":9,\"ddd\":3}\n",
      "E\t[\"e\",\"b\",\"f\"]\t{\"bbb\":6,\"iii\":3,\"hhh\":5,\"fff\":4,\"ggg\":9,\"ddd\":2}\n",
      "D\t[\"g\",\"a\"]\t{\"hhh\":4,\"jjj\":5,\"ccc\":9}\n",
      "E\t[\"e\",\"c\",\"f\",\"a\"]\t{\"ccc\":1,\"iii\":6,\"fff\":9}\n",
      "E\t[\"e\",\"a\"]\t{\"bbb\":9,\"aaa\":3,\"fff\":1}\n",
      "E\t[\"e\",\"f\"]\t{\"ddd\":9,\"iii\":2,\"aaa\":4}\n",
      "E\t[\"c\",\"b\",\"g\"]\t{\"ccc\":5,\"fff\":8,\"iii\":7}\n",
      "D\t[\"c\",\"f\",\"a\"]\t{\"eee\":3,\"jjj\":2,\"ddd\":7}\n",
      "A\t[\"f\",\"a\",\"d\"]\t{\"jjj\":1,\"ggg\":0,\"ccc\":7,\"ddd\":9,\"bbb\":3}\n",
      "E\t[\"c\",\"d\"]\t{\"jjj\":6,\"ccc\":0,\"aaa\":1,\"hhh\":9,\"iii\":7,\"ggg\":8}\n",
      "E\t[\"e\",\"d\",\"c\"]\t{\"fff\":3,\"eee\":6,\"iii\":4,\"bbb\":7,\"ddd\":0,\"ccc\":1}\n",
      "A\t[\"a\",\"e\",\"f\"]\t{\"fff\":0,\"ddd\":5,\"ccc\":4}\n",
      "E\t[\"c\",\"a\",\"g\"]\t{\"ggg\":6,\"hhh\":3,\"ddd\":9,\"ccc\":0,\"jjj\":7}\n",
      "A\t[\"f\",\"e\"]\t{\"hhh\":6,\"jjj\":0,\"eee\":5,\"iii\":7,\"ccc\":3}\n",
      "C\t[\"f\",\"c\",\"a\",\"g\"]\t{\"eee\":1,\"fff\":4,\"aaa\":2,\"ccc\":7,\"ggg\":0,\"ddd\":6}\n",
      "A\t[\"b\",\"f\"]\t{\"ccc\":6,\"aaa\":9,\"eee\":5,\"ddd\":0,\"bbb\":3}\n",
      "D\t[\"b\",\"f\"]\t{\"bbb\":7,\"hhh\":1,\"aaa\":6,\"iii\":4,\"fff\":9,\"ddd\":5}\n",
      "E\t[\"a\",\"c\"]\t{\"fff\":3,\"ccc\":1,\"ggg\":2,\"eee\":5}\n",
      "B\t[\"b\",\"f\",\"c\"]\t{\"iii\":7,\"ggg\":3,\"ddd\":0,\"jjj\":8,\"hhh\":5,\"ccc\":1}\n",
      "B\t[\"f\",\"a\",\"e\"]\t{\"hhh\":6,\"ccc\":3,\"jjj\":0,\"bbb\":8,\"ddd\":7}\n",
      "D\t[\"a\",\"f\"]\t{\"aaa\":0,\"fff\":5,\"ddd\":3}\n",
      "B\t[\"c\",\"a\"]\t{\"ddd\":5,\"jjj\":2,\"iii\":7,\"ccc\":0,\"bbb\":4}\n",
      "C\t[\"c\",\"a\",\"e\",\"f\"]\t{\"eee\":0,\"fff\":2,\"hhh\":6}\n",
      "E\t[\"e\",\"d\"]\t{\"fff\":9,\"iii\":2,\"eee\":0}\n",
      "E\t[\"f\",\"a\",\"d\"]\t{\"hhh\":8,\"ggg\":3,\"jjj\":5}\n",
      "Time taken: 3.264 seconds, Fetched: 40 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive \n",
    "SELECT * FROM t0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS dataTable; \n",
      "OK\n",
      "Time taken: 0.407 seconds\n",
      "CREATE TABLE dataTable (clave STRING, valor INT);\n",
      "OK\n",
      "Time taken: 0.048 seconds\n",
      "INSERT OVERWRITE TABLE dataTable \n",
      "SELECT explode(c3) FROM t0;\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = root_20200228152959_0edd50e2-50ca-4be4-8cb4-b2c5277af9c9\n",
      "Total jobs = 3\n",
      "Launching Job 1 out of 3\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1582902829190_0002, Tracking URL = http://5c70801abc21:8088/proxy/application_1582902829190_0002/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1582902829190_0002\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2020-02-28 15:30:08,427 Stage-1 map = 0%,  reduce = 0%\n",
      "2020-02-28 15:30:14,034 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.51 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 510 msec\n",
      "Ended Job = job_1582902829190_0002\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to directory hdfs://0.0.0.0:9000/user/hive/warehouse/datatable/.hive-staging_hive_2020-02-28_15-29-59_145_2679308920510745165-1/-ext-10000\n",
      "Loading data to table default.datatable\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 2.51 sec   HDFS Read: 6039 HDFS Write: 1125 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 2 seconds 510 msec\n",
      "OK\n",
      "Time taken: 18.0 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DROP TABLE IF EXISTS dataTable; \n",
    "CREATE TABLE dataTable (clave STRING, valor INT);\n",
    "INSERT OVERWRITE TABLE dataTable \n",
    "SELECT explode(c3) FROM t0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT clave FROM dataTable LIMIT 10;\n",
      "OK\n",
      "jjj\n",
      "bbb\n",
      "ddd\n",
      "ggg\n",
      "hhh\n",
      "ccc\n",
      "ddd\n",
      "aaa\n",
      "hhh\n",
      "ddd\n",
      "Time taken: 0.143 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT clave FROM dataTable LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS word_counts;\n",
      "OK\n",
      "Time taken: 0.006 seconds\n",
      "CREATE TABLE word_counts\n",
      "AS\n",
      "    SELECT word,count(1) AS count\n",
      "    FROM\n",
      "        (SELECT clave AS word FROM dataTable) w\n",
      "GROUP BY\n",
      "    word\n",
      "ORDER BY\n",
      "    word;\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = root_20200228153130_9792265e-5cb5-4d4c-a420-e3879729959f\n",
      "Total jobs = 2\n",
      "Launching Job 1 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1582902829190_0003, Tracking URL = http://5c70801abc21:8088/proxy/application_1582902829190_0003/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1582902829190_0003\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2020-02-28 15:31:39,000 Stage-1 map = 0%,  reduce = 0%\n",
      "2020-02-28 15:31:45,690 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.79 sec\n",
      "2020-02-28 15:31:52,225 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.36 sec\n",
      "MapReduce Total cumulative CPU time: 4 seconds 360 msec\n",
      "Ended Job = job_1582902829190_0003\n",
      "Launching Job 2 out of 2\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1582902829190_0004, Tracking URL = http://5c70801abc21:8088/proxy/application_1582902829190_0004/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1582902829190_0004\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2020-02-28 15:32:05,838 Stage-2 map = 0%,  reduce = 0%\n",
      "2020-02-28 15:32:11,397 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.25 sec\n",
      "2020-02-28 15:32:16,869 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.0 sec\n",
      "MapReduce Total cumulative CPU time: 3 seconds 0 msec\n",
      "Ended Job = job_1582902829190_0004\n",
      "Moving data to directory hdfs://0.0.0.0:9000/user/hive/warehouse/word_counts\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.36 sec   HDFS Read: 8199 HDFS Write: 316 SUCCESS\n",
      "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 3.0 sec   HDFS Read: 5467 HDFS Write: 145 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 7 seconds 360 msec\n",
      "OK\n",
      "Time taken: 48.089 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DROP TABLE IF EXISTS word_counts;\n",
    "CREATE TABLE word_counts\n",
    "AS\n",
    "    SELECT word,count(1) AS count\n",
    "    FROM\n",
    "        (SELECT clave AS word FROM dataTable) w\n",
    "GROUP BY\n",
    "    word\n",
    "ORDER BY\n",
    "    word;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM word_counts;\n",
      "OK\n",
      "aaa\t13\n",
      "bbb\t16\n",
      "ccc\t23\n",
      "ddd\t23\n",
      "eee\t15\n",
      "fff\t20\n",
      "ggg\t13\n",
      "hhh\t16\n",
      "iii\t18\n",
      "jjj\t18\n",
      "Time taken: 0.175 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT * FROM word_counts;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%hive_quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwxrwx---   - root supergroup          0 2020-02-28 15:14 /tmp/hadoop-yarn\n",
      "drwxrwxrwx   - root supergroup          0 2020-02-28 15:19 /tmp/hive\n"
     ]
    }
   ],
   "source": [
    "## Copia Archivos a sistema HDFS\n",
    "##\n",
    "## Se usan un directorio temporal en el HDFS. La siguiente\n",
    "## instrucción muestra el contenido del dicho directorio\n",
    "##\n",
    "!hdfs dfs -ls /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Crea la carpeta wordcount en el hdfs\n",
    "##\n",
    "!hdfs dfs -mkdir /tmp/wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "drwxrwx---   - root supergroup          0 2020-02-28 15:14 /tmp/hadoop-yarn\n",
      "drwxrwxrwx   - root supergroup          0 2020-02-28 15:19 /tmp/hive\n",
      "drwxr-xr-x   - root supergroup          0 2020-02-28 15:33 /tmp/wordcount\n"
     ]
    }
   ],
   "source": [
    "## Verifica la creación de la carpeta\n",
    "##\n",
    "!hdfs dfs -ls /tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Copia los archvios del directorio local wordcount/\n",
    "## al directorio /tmp/wordcount/ en el hdfs\n",
    "##\n",
    "!hdfs dfs -copyFromLocal 'data.tsv'  /tmp/wordcount/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   1 root supergroup       1394 2020-02-28 15:33 /tmp/wordcount/data.tsv\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Verifica que los archivos esten copiados\n",
    "## en el hdfs\n",
    "##\n",
    "!hdfs dfs -ls /tmp/wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "## Generación del script y ajuste del código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wordcount.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount.hql\n",
    "\n",
    "DROP TABLE IF EXISTS t0;\n",
    "CREATE TABLE t0 (\n",
    "    c1 STRING,\n",
    "    c2 ARRAY<CHAR(1)>, \n",
    "    c3 MAP<STRING, INT>\n",
    "    )\n",
    "    ROW FORMAT DELIMITED \n",
    "        FIELDS TERMINATED BY '\\t'\n",
    "        COLLECTION ITEMS TERMINATED BY ','\n",
    "        MAP KEYS TERMINATED BY '#'\n",
    "        LINES TERMINATED BY '\\n';\n",
    "LOAD DATA LOCAL INPATH 'data.tsv' INTO TABLE t0;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DROP TABLE IF EXISTS dataTable; \n",
    "CREATE TABLE dataTable (clave STRING, valor INT);\n",
    "INSERT OVERWRITE TABLE dataTable \n",
    "SELECT explode(c3) FROM t0;\n",
    "\n",
    "DROP TABLE IF EXISTS word_counts;\n",
    "CREATE TABLE word_counts\n",
    "AS\n",
    "    SELECT word,count(1) AS count\n",
    "    FROM\n",
    "        (SELECT clave AS word FROM dataTable) w\n",
    "GROUP BY\n",
    "    word\n",
    "ORDER BY\n",
    "    word;\n",
    "\n",
    "\n",
    "INSERT OVERWRITE DIRECTORY '/tmp/output'\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "\n",
    "SELECT * FROM word_counts;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n"
     ]
    }
   ],
   "source": [
    "## Ejecucion\n",
    "!hive -S -e 'source wordcount.hql'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/000000_0\n"
     ]
    }
   ],
   "source": [
    "### Copia de los resultados a la maquina local (vagrant)\n",
    "!hadoop fs -copyToLocal /tmp/output output\n",
    "!ls output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaa,13\n",
      "bbb,16\n",
      "ccc,23\n",
      "ddd,23\n",
      "eee,15\n",
      "fff,20\n",
      "ggg,13\n",
      "hhh,16\n",
      "iii,18\n",
      "jjj,18\n"
     ]
    }
   ],
   "source": [
    "!cat output/000000_0 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
